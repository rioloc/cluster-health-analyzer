# LightSpeed Evaluation Framework - Conversation Scenario Test Data
# Based on critical control plane failure scenario from conversation.txt

- conversation_group_id: "openshift_critical_control_plane_failure"
  description: "Evaluation based on critical control plane failure scenario with etcd issues, scheduler/controller manager down, and application impact"

  conversation_metrics: []
  conversation_metrics_metadata: {}

  turns:
    # Test Case 1: Basic health check
    - turn_id: "basic_cluster_health_check"
      query: "How is my OpenShift cluster doing today?"
      response: null
      contexts:
        - "Cluster health is determined by analyzing current incidents and their severity levels."
        - "Critical incidents indicate severe problems requiring immediate attention."
        - "Multiple incidents suggest systemic issues affecting cluster stability."
      expected_response: "Your cluster is experiencing significant health issues with multiple active incidents affecting critical infrastructure components."

      expected_tool_calls:
        -
          - tool_name: get_incidents
            arguments:
              max_age_hours: "\\d+"

      turn_metrics:
        - "ragas:response_relevancy"
        - "custom:answer_correctness"

      turn_metrics_metadata:
        "ragas:response_relevancy":
          threshold: 0.35
        "custom:answer_correctness":
          threshold: 0.70

    # Test Case 2: Critical incident identification with specific group_id
    - turn_id: "identify_critical_issues"
      query: "What are the most serious problems in my cluster right now?"
      response: null
      contexts:
        - "Critical severity incidents represent the most serious cluster problems."
        - "Incident a96081f6-542e-4a60-b3bb-6bb7d5b2e169 is a critical incident affecting control plane."
        - "Control plane failures have cluster-wide impact and require immediate attention."
      expected_response: "The most serious issue is a critical incident a96081f6-542e-4a60-b3bb-6bb7d5b2e169 affecting your control plane components, including etcd failures and scheduler/controller manager being down."

      expected_tool_calls:
        -
          - tool_name: get_incidents
            arguments:
              max_age_hours: "\\d+"

      turn_metrics:
        - "ragas:response_relevancy"
        - "custom:answer_correctness"

      turn_metrics_metadata:
        "ragas:response_relevancy":
          threshold: 0.70
        "custom:answer_correctness":
          threshold: 0.60

    # Test Case 3: etcd specific inquiry with alert names
    - turn_id: "etcd_health_status"
      query: "Is my etcd cluster healthy?"
      response: null
      contexts:
        - "etcd health issues are indicated by alerts like etcdHighNumberOfFailedProposals, etcdHighFsyncDurations, etcdGRPCRequestsSlow."
        - "etcd problems are part of the critical incident a96081f6-542e-4a60-b3bb-6bb7d5b2e169."
        - "etcd failures can cause cascading issues throughout the cluster."
      expected_response: "No, your etcd cluster is not healthy. It shows critical issues like high failed proposals, slow gRPC requests, and high fsync durations, indicating etcd performance degradation that impacts cluster stability."

      expected_tool_calls:
        -
          - tool_name: get_incidents
            arguments:
              max_age_hours: "\\d+"

      turn_metrics:
        - "ragas:response_relevancy"
        - "custom:answer_correctness"

      turn_metrics_metadata:
        "ragas:response_relevancy":
          threshold: 0.68
        "custom:answer_correctness":
          threshold: 0.60

    # Test Case 3b: etcd specific inquiry alternative
    - turn_id: "etcd_health_status_alt"
      query: "Is my etcd cluster healthy?"
      response: null
      contexts:
        - "etcd health issues are indicated by alerts like etcdHighNumberOfFailedProposals, etcdHighFsyncDurations, etcdGRPCRequestsSlow."
        - "etcd problems are part of the critical incident a96081f6-542e-4a60-b3bb-6bb7d5b2e169."
        - "etcd failures can cause cascading issues throughout the cluster."
      expected_response: "Your etcd cluster is currently unhealthy with critical degradation, frequent leader changes, and unhealthy members, causing cluster instability and control plane disruptions."

      expected_tool_calls:
        -
          - tool_name: get_incidents
            arguments:
              max_age_hours: "\\d+"

      turn_metrics:
        - "ragas:response_relevancy"
        - "custom:answer_correctness"

      turn_metrics_metadata:
        "ragas:response_relevancy":
          threshold: 0.63
        "custom:answer_correctness":
          threshold: 0.50

    # Test Case 4: Control plane components status with specific alerts
    - turn_id: "control_plane_status"
      query: "Are my Kubernetes control plane components running properly?"
      response: null
      contexts:
        - "Control plane components include kube-scheduler, kube-controller-manager, kube-apiserver."
        - "KubeSchedulerDown and KubeControllerManagerDown alerts indicate critical failures."
        - "These components are essential for cluster operations and pod management."
      expected_response: "Your control plane components are experiencing critical failures - KubeSchedulerDown and KubeControllerManagerDown alerts are active, and the kube-apiserver has KubeAPITerminatedRequests issues, all part of incident a96081f6-542e-4a60-b3bb-6bb7d5b2e169."

      expected_tool_calls:
        -
          - tool_name: get_incidents
            arguments:
              max_age_hours: "\\d+"

      turn_metrics:
        - "ragas:response_relevancy"
        - "custom:answer_correctness"

      turn_metrics_metadata:
        "ragas:response_relevancy":
          threshold: 0.70
        "custom:answer_correctness":
          threshold: 0.60

    # Test Case 5: Application namespace specific check
    - turn_id: "application_namespace_health"
      query: "How are my applications running in the my-app namespace?"
      response: null
      contexts:
        - "Application health in my-app namespace is affected by KubePodNotReady alert."
        - "Pod readiness issues are often symptoms of underlying infrastructure problems."
        - "The my-app namespace issues are connected to the broader control plane failures."
      expected_response: "Your applications in the my-app namespace are experiencing issues with pods not being ready, which appears to be related to the broader control plane failures affecting your cluster."

      expected_tool_calls:
        -
          - tool_name: get_incidents
            arguments:
              max_age_hours: "\\d+"

      turn_metrics:
        - "ragas:response_relevancy"
        - "custom:answer_correctness"

      turn_metrics_metadata:
        "ragas:response_relevancy":
          threshold: 0.50
        "custom:answer_correctness":
          threshold: 0.65

    # Test Case 6: Storage subsystem check
    - turn_id: "storage_health_check"
      query: "Is my cluster storage working correctly?"
      response: null
      contexts:
        - "Storage issues are indicated by KubePersistentVolumeErrors alert."
        - "Storage problems can prevent applications from starting or accessing data."
        - "Storage issues are part of the broader infrastructure problems."
      expected_response: "Your cluster storage is experiencing persistent volume errors, which are contributing to the overall infrastructure issues affecting your cluster."

      expected_tool_calls:
        -
          - tool_name: get_incidents
            arguments:
              max_age_hours: "\\d+"

      turn_metrics:
        - "ragas:response_relevancy"
        - "custom:answer_correctness"

      turn_metrics_metadata:
        "ragas:response_relevancy":
          threshold: 0.49
        "custom:answer_correctness":
          threshold: 0.65

    # Test Case 7: Monitoring configuration check with specific incident and alert
    - turn_id: "monitoring_alerting_status"
      query: "Is my cluster monitoring and alerting configured properly?"
      response: null
      contexts:
        - "Monitoring configuration issues are indicated by AlertmanagerReceiversNotConfigured alert."
        - "Incident 95eed194-2778-467d-8fd1-c2128b164151 contains this monitoring alert."
        - "Proper alerting configuration is important for operational awareness."
      expected_response: "Your monitoring system has a configuration issue - incident 95eed194-2778-467d-8fd1-c2128b164151 shows the AlertmanagerReceiversNotConfigured alert, indicating that Alertmanager receivers are not configured properly."

      expected_tool_calls:
        -
          - tool_name: get_incidents
            arguments:
              max_age_hours: "\\d+"

      turn_metrics:
        - "ragas:response_relevancy"
        - "custom:answer_correctness"

      turn_metrics_metadata:
        "ragas:response_relevancy":
          threshold: 0.51
        "custom:answer_correctness":
          threshold: 0.70

    # Test Case 8: Cluster update status with specific incident
    - turn_id: "cluster_update_availability"
      query: "Should I update my cluster to a newer version?"
      response: null
      contexts:
        - "UpdateAvailable alert indicates a cluster update is available."
        - "Incident 9ce79082-3803-4ba0-a130-97994fd7966b contains the update information."
        - "Updates should typically be postponed until critical issues are resolved."
      expected_response: "There is an update available for your cluster, but before proceeding, assess the risks given your current critical etcd and control plane issues. Stabilize your cluster first before updating unless you have a strong reason like critical CVE fixes."

      expected_tool_calls:
        -
          - tool_name: get_incidents
            arguments:
              max_age_hours: "\\d+"

      turn_metrics:
        - "ragas:response_relevancy"
        - "custom:answer_correctness"

      turn_metrics_metadata:
        "ragas:response_relevancy":
          threshold: 0.50
        "custom:answer_correctness":
          threshold: 0.65

    # Test Case 8b: Cluster update status alternative
    - turn_id: "cluster_update_availability_alt"
      query: "Should I update my cluster to a newer version?"
      response: null
      contexts:
        - "UpdateAvailable alert indicates a cluster update is available."
        - "Incident 9ce79082-3803-4ba0-a130-97994fd7966b contains the update information."
        - "Updates should typically be postponed until critical issues are resolved."
      expected_response: "Before updating, check if the target version is recommended. Given your cluster's current critical issues, updating might help if the new version fixes relevant bugs, but only proceed if officially recommended and after resolving existing critical problems."

      expected_tool_calls:
        -
          - tool_name: get_incidents
            arguments:
              max_age_hours: "\\d+"

      turn_metrics:
        - "ragas:response_relevancy"
        - "custom:answer_correctness"

      turn_metrics_metadata:
        "ragas:response_relevancy":
          threshold: 0.50
        "custom:answer_correctness":
          threshold: 0.60

    # Test Case 9: Incident correlation
    - turn_id: "incident_relationship_analysis"
      query: "Are these cluster problems related to each other?"
      response: null
      contexts:
        - "Multiple alerts in incident a96081f6-542e-4a60-b3bb-6bb7d5b2e169 suggest related failures."
        - "etcd issues can cause cascading failures in other control plane components."
        - "Infrastructure failures often have cascading effects on applications."
      expected_response: "Yes, the cluster problems in incident a96081f6-542e-4a60-b3bb-6bb7d5b2e169 are highly related. The etcdHighNumberOfFailedProposals, etcdHighFsyncDurations, and etcdGRPCRequestsSlow alerts are causing cascading failures in KubeSchedulerDown, KubeControllerManagerDown, and affecting KubePersistentVolumeErrors and KubePodNotReady throughout the cluster."

      expected_tool_calls:
        -
          - tool_name: get_incidents
            arguments:
              max_age_hours: "\\d+"

      turn_metrics:
        - "ragas:response_relevancy"
        - "custom:answer_correctness"

      turn_metrics_metadata:
        "ragas:response_relevancy":
          threshold: 0.58
        "custom:answer_correctness":
          threshold: 0.70

    # Test Case 10: Prioritization guidance
    - turn_id: "issue_prioritization_guidance"
      query: "Which problems should I fix first?"
      response: null
      contexts:
        - "Critical incidents should be addressed before warning or info level incidents."
        - "Control plane stability is foundational to cluster health."
        - "etcd issues often need to be resolved before other components can be fixed."
      expected_response: "You should prioritize fixing incident a96081f6-542e-4a60-b3bb-6bb7d5b2e169 first, starting with the etcdHighNumberOfFailedProposals and related etcd alerts, then addressing KubeSchedulerDown and KubeControllerManagerDown. Incidents 95eed194-2778-467d-8fd1-c2128b164151 and 9ce79082-3803-4ba0-a130-97994fd7966b can wait until the critical infrastructure is stable."

      expected_tool_calls:
        -
          - tool_name: get_incidents
            arguments:
              max_age_hours: "\\d+"

      turn_metrics:
        - "ragas:response_relevancy"
        - "custom:answer_correctness"

      turn_metrics_metadata:
        "ragas:response_relevancy":
          threshold: 0.31
        "custom:answer_correctness":
          threshold: 0.70

    # Test Case 11: Specific incident ID verification
    - turn_id: "specific_incident_verification"
      query: "Can you tell me the exact incident ID for the critical control plane issues?"
      response: null
      contexts:
        - "The critical control plane incident has a specific UUID: a96081f6-542e-4a60-b3bb-6bb7d5b2e169."
        - "Incident IDs are important for tracking and referencing specific problems."
        - "This incident contains multiple related alerts affecting control plane components."
      expected_response: "The critical control plane incident has ID a96081f6-542e-4a60-b3bb-6bb7d5b2e169, which contains the etcd, scheduler, controller manager, and related infrastructure alerts."

      expected_tool_calls:
        -
          - tool_name: get_incidents
            arguments:
              max_age_hours: "\\d+"

      turn_metrics:
        - "ragas:response_relevancy"
        - "custom:answer_correctness"

      turn_metrics_metadata:
        "ragas:response_relevancy":
          threshold: 0.70
        "custom:answer_correctness":
          threshold: 0.75

    # Test Case 13: Query for exact incident ID - a96081f6-542e-4a60-b3bb-6bb7d5b2e169
    - turn_id: "query_exact_incident_critical"
      query: "Show me details for incident a96081f6-542e-4a60-b3bb-6bb7d5b2e169"
      response: null
      contexts:
        - "Incident a96081f6-542e-4a60-b3bb-6bb7d5b2e169 is a critical incident with multiple alerts."
        - "This incident contains etcd, scheduler, controller manager, and storage alerts."
        - "The incident started with etcd problems and cascaded to other components."
      expected_response: "Incident a96081f6-542e-4a60-b3bb-6bb7d5b2e169 is a critical infrastructure incident containing 8 alerts: etcdHighNumberOfFailedProposals, etcdHighFsyncDurations, etcdGRPCRequestsSlow, KubeSchedulerDown, KubeControllerManagerDown, KubeAPITerminatedRequests, KubePersistentVolumeErrors, and KubePodNotReady across core and workload layers."

      expected_tool_calls:
        -
          - tool_name: get_incidents
            arguments:
              max_age_hours: "\\d+"

      turn_metrics:
        - "ragas:response_relevancy"
        - "custom:answer_correctness"

      turn_metrics_metadata:
        "ragas:response_relevancy":
          threshold: 0.75
        "custom:answer_correctness":
          threshold: 0.80

    # Test Case 14: Query for exact incident ID - 9ce79082-3803-4ba0-a130-97994fd7966b
    - turn_id: "query_exact_incident_update"
      query: "What is in incident 9ce79082-3803-4ba0-a130-97994fd7966b?"
      response: null
      contexts:
        - "Incident 9ce79082-3803-4ba0-a130-97994fd7966b contains the UpdateAvailable alert."
        - "This incident is info severity and indicates a cluster update is available."
        - "The incident is currently silenced according to the scenario data."
      expected_response: "Incident 9ce79082-3803-4ba0-a130-97994fd7966b contains a single UpdateAvailable alert from the openshift-cluster-version namespace, indicating that a cluster update is available on the stable-4.19 channel. This is an info-level alert that is currently silenced."

      expected_tool_calls:
        -
          - tool_name: get_incidents
            arguments:
              max_age_hours: "\\d+"

      turn_metrics:
        - "ragas:response_relevancy"
        - "custom:answer_correctness"

      turn_metrics_metadata:
        "ragas:response_relevancy":
          threshold: 0.75
        "custom:answer_correctness":
          threshold: 0.70

    # Test Case 15: Query for exact incident ID - 95eed194-2778-467d-8fd1-c2128b164151
    - turn_id: "query_exact_incident_alertmanager"
      query: "Tell me about incident 95eed194-2778-467d-8fd1-c2128b164151"
      response: null
      contexts:
        - "Incident 95eed194-2778-467d-8fd1-c2128b164151 contains AlertmanagerReceiversNotConfigured alert."
        - "This incident indicates a monitoring configuration problem."
        - "The alert is warning severity and affects the monitoring component."
      expected_response: "Incident 95eed194-2778-467d-8fd1-c2128b164151 contains the AlertmanagerReceiversNotConfigured alert from the openshift-monitoring namespace. This warning-level alert indicates that Alertmanager receivers are not properly configured, which affects your cluster's monitoring and alerting capabilities."

      expected_tool_calls:
        -
          - tool_name: get_incidents
            arguments:
              max_age_hours: "\\d+"

      turn_metrics:
        - "ragas:response_relevancy"
        - "custom:answer_correctness"

      turn_metrics_metadata:
        "ragas:response_relevancy":
          threshold: 0.75
        "custom:answer_correctness":
          threshold: 0.75

    # Test Case 16: Query for exact alert name - etcdHighNumberOfFailedProposals
    - turn_id: "query_exact_alert_etcd_proposals"
      query: "What can you tell me about the etcdHighNumberOfFailedProposals alert?"
      response: null
      contexts:
        - "etcdHighNumberOfFailedProposals alert indicates critical etcd performance issues."
        - "This alert is part of incident a96081f6-542e-4a60-b3bb-6bb7d5b2e169."
        - "Failed proposals in etcd can cause cluster instability and control plane issues."
      expected_response: "The etcdHighNumberOfFailedProposals alert is a critical alert from the openshift-etcd namespace that indicates etcd is experiencing a high number of failed consensus proposals. This alert is part of incident a96081f6-542e-4a60-b3bb-6bb7d5b2e169 and suggests serious etcd performance issues that can cause cluster instability."

      expected_tool_calls:
        -
          - tool_name: get_incidents
            arguments:
              max_age_hours: "\\d+"

      turn_metrics:
        - "ragas:response_relevancy"
        - "custom:answer_correctness"

      turn_metrics_metadata:
        "ragas:response_relevancy":
          threshold: 0.70
        "custom:answer_correctness":
          threshold: 0.75

    # Test Case 17: Query for exact alert name - KubeSchedulerDown
    - turn_id: "query_exact_alert_scheduler_down"
      query: "Is the KubeSchedulerDown alert currently firing?"
      response: null
      contexts:
        - "KubeSchedulerDown alert indicates the Kubernetes scheduler is not functioning."
        - "This alert is critical severity and part of incident a96081f6-542e-4a60-b3bb-6bb7d5b2e169."
        - "When the scheduler is down, new pods cannot be scheduled to nodes."
      expected_response: "Yes, the KubeSchedulerDown alert is currently firing. This is a critical alert from the openshift-kube-scheduler namespace, part of incident a96081f6-542e-4a60-b3bb-6bb7d5b2e169. When this alert fires, it means the Kubernetes scheduler is down and new pods cannot be scheduled in your cluster."

      expected_tool_calls:
        -
          - tool_name: get_incidents
            arguments:
              max_age_hours: "\\d+"

      turn_metrics:
        - "ragas:response_relevancy"
        - "custom:answer_correctness"

      turn_metrics_metadata:
        "ragas:response_relevancy":
          threshold: 0.75
        "custom:answer_correctness":
          threshold: 0.70

    # Test Case 18: Query for exact alert name - AlertmanagerReceiversNotConfigured
    - turn_id: "query_exact_alert_alertmanager_config"
      query: "What does the AlertmanagerReceiversNotConfigured alert mean?"
      response: null
      contexts:
        - "AlertmanagerReceiversNotConfigured alert indicates monitoring configuration issues."
        - "This alert is part of incident 95eed194-2778-467d-8fd1-c2128b164151."
        - "Proper receiver configuration is needed for alert notifications."
      expected_response: "The AlertmanagerReceiversNotConfigured alert is a warning-level alert from the openshift-monitoring namespace, part of incident 95eed194-2778-467d-8fd1-c2128b164151. This alert means that Alertmanager receivers are not properly configured, which prevents alert notifications from being delivered to the intended destinations."

      expected_tool_calls:
        -
          - tool_name: get_incidents
            arguments:
              max_age_hours: "\\d+"

      turn_metrics:
        - "ragas:response_relevancy"
        - "custom:answer_correctness"

      turn_metrics_metadata:
        "ragas:response_relevancy":
          threshold: 0.70
        "custom:answer_correctness":
          threshold: 0.75

    # Test Case 19: Query for exact alert name - UpdateAvailable
    - turn_id: "query_exact_alert_update_available"
      query: "Tell me about the UpdateAvailable alert status"
      response: null
      contexts:
        - "UpdateAvailable alert indicates a cluster update is available."
        - "This alert is part of incident 9ce79082-3803-4ba0-a130-97994fd7966b."
        - "The alert shows channel stable-4.19 has an available update."
        - "This alert is currently silenced in the scenario."
      expected_response: "The UpdateAvailable alert is an info-level alert from the openshift-cluster-version namespace, part of incident 9ce79082-3803-4ba0-a130-97994fd7966b. This alert indicates that a cluster update is available on the stable-4.19 channel. The alert is currently silenced, suggesting the update notification is acknowledged but not yet acted upon."

      expected_tool_calls:
        -
          - tool_name: get_incidents
            arguments:
              max_age_hours: "\\d+"

      turn_metrics:
        - "ragas:response_relevancy"
        - "custom:answer_correctness"

      turn_metrics_metadata:
        "ragas:response_relevancy":
          threshold: 0.70
        "custom:answer_correctness":
          threshold: 0.70

    # Test Case 20: Mixed query - specific incident and alert details
    - turn_id: "query_mixed_incident_alert_details"
      query: "Show me all etcd-related alerts in incident a96081f6-542e-4a60-b3bb-6bb7d5b2e169"
      response: null
      contexts:
        - "Incident a96081f6-542e-4a60-b3bb-6bb7d5b2e169 contains three etcd-related alerts."
        - "The etcd alerts are: etcdHighNumberOfFailedProposals, etcdHighFsyncDurations, etcdGRPCRequestsSlow."
        - "All etcd alerts are from the openshift-etcd namespace and affect the etcd component."
      expected_response: "Incident a96081f6-542e-4a60-b3bb-6bb7d5b2e169 contains three etcd-related alerts from the openshift-etcd namespace: etcdHighNumberOfFailedProposals (critical severity), etcdHighFsyncDurations (warning), and etcdGRPCRequestsSlow (warning). These alerts indicate comprehensive etcd performance problems affecting consensus proposals, disk sync operations, and gRPC request handling."

      expected_tool_calls:
        -
          - tool_name: get_incidents
            arguments:
              max_age_hours: "\\d+"

      turn_metrics:
        - "ragas:response_relevancy"
        - "custom:answer_correctness"

      turn_metrics_metadata:
        "ragas:response_relevancy":
          threshold: 0.75
        "custom:answer_correctness":
          threshold: 0.80

    # Test Case 21: Silenced incident detection
    - turn_id: "query_silenced_incidents"
      query: "Are there any silenced incidents in my cluster?"
      response: null
      contexts:
        - "Incident 9ce79082-3803-4ba0-a130-97994fd7966b contains a silenced alert."
        - "The UpdateAvailable alert in this incident is silenced."
        - "Silenced alerts indicate acknowledged but unresolved issues."
      expected_response: "Yes, incident 9ce79082-3803-4ba0-a130-97994fd7966b contains a silenced alert. The UpdateAvailable alert from the openshift-cluster-version namespace is currently silenced, indicating that the available cluster update notification has been acknowledged but the update has not been applied yet."

      expected_tool_calls:
        -
          - tool_name: get_incidents
            arguments:
              max_age_hours: "\\d+"

      turn_metrics:
        - "ragas:response_relevancy"
        - "custom:answer_correctness"

      turn_metrics_metadata:
        "ragas:response_relevancy":
          threshold: 0.65
        "custom:answer_correctness":
          threshold: 0.70

    # Test Case 12: Complete alert enumeration for critical incident
    - turn_id: "complete_alert_enumeration"
      query: "List all the specific alerts that are part of the critical infrastructure incident"
      response: null
      contexts:
        - "Incident a96081f6-542e-4a60-b3bb-6bb7d5b2e169 contains exactly these alerts: etcdHighNumberOfFailedProposals, etcdHighFsyncDurations, etcdGRPCRequestsSlow, KubeControllerManagerDown, KubeSchedulerDown, KubeAPITerminatedRequests, KubePersistentVolumeErrors, KubePodNotReady."
        - "Each alert name should be mentioned specifically for complete incident analysis."
        - "The response should include the incident ID and all associated alert names."
      expected_response: "The critical incident a96081f6-542e-4a60-b3bb-6bb7d5b2e169 contains these specific alerts: etcdHighNumberOfFailedProposals, etcdHighFsyncDurations, etcdGRPCRequestsSlow, KubeControllerManagerDown, KubeSchedulerDown, KubeAPITerminatedRequests, KubePersistentVolumeErrors, and KubePodNotReady."

      expected_tool_calls:
        -
          - tool_name: get_incidents
            arguments:
              max_age_hours: "\\d+"

      turn_metrics:
        - "ragas:response_relevancy"
        - "custom:answer_correctness"

      turn_metrics_metadata:
        "ragas:response_relevancy":
          threshold: 0.70
        "custom:answer_correctness":
          threshold: 0.30